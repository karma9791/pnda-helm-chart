## ----------------------------
## PNDA Components 
## ----------------------------

serviceAccountName: pnda
roleName: pnda-role
roleBindingName: pnda

# prometheusOperator integration disabled by default
# enable rbac access and each serviceMonitor with enabled field 
prometheus:
  # prometheus-k8s serviceAccount and namespace to configure RBAC access to pnda metrics endpoints
  rbac:
    enabled: true
    serviceAccount: prometheus-k8s
    namespace: monitoring
  #Configuration for each prometheusOperator service Monitor
  serviceMonitor:
    hdfs:
      enabled: true
      # If namespace is empty: serviceMonitor is deployed in {{ .Release.namespace }}
      namespace: 
      interval: 15s
    hbase:
      enabled: true
      namespace:
      interval: 15s
    kafka:
      enabled: true
      namespace: 
      interval: 15s
    spark:
      enabled: true
      namespace: 
      interval: 15s

redis:
  enabled: true
  fullnameOverride: redis
  # HA disable by default
  cluster:
    enabled: false
  usePassword: false

consoleBackendDataLogger:
  fullnameOverride: console-backend-data-logger
  image:
    repository: pnda/console-backend-data-logger
    tag: 2.1.1
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 3001
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"
consoleBackendDataManager:
  fullnameOverride: console-backend-data-manager
  image:
    repository: pnda/console-backend-data-manager
    tag: 2.1.1
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 3123
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"
consoleFrontend:
  fullnameOverride: console-frontend
  image:
    repository:  pnda/console-frontend
    tag: 2.1.2
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
  conf:
    datasets:
      hidden:
      - '"testbot"'
      - '"+tmp"'
    topics:
      hidden: 
      - '"avro.internal.testbot"'
      - '"__consumer_offsets"'
      - '"_schemas"'
      - '"__connect.offset"'
      - '"__connect.status"'
      - '"__connect.config"'
      - '"__confluent.support.metrics"'
    logLevel: INFO
    clusterName:
    version:
    edgeNode: ""
    hdfsLink: "http://hdfs.127-0-0-1.nip.io"
    kafkaManagerLink: "http://kafka-manager.127-0-0-1.nip.io"
    grafanaLink: "http://grafana.127-0-0-1.nip.io"
    kibanaLink: "http://kibana.127-0-0-1.nip.io"
    jupyterLink: "http://notebooks.127-0-0-1.nip.io"
    httpfsLink: "http://httpfs.127-0-0-1.nip.io"
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"
  ingress:
    enabled: true
    annotations: {}
    path: /
    hosts:
      - console.127-0-0-1.nip.io
dataService:
  fullnameOverride: data-service
  image:
    repository: pnda/data-service
    tag: 0.4.2
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 7000
  conf:
    logLevel: INFO
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"
deploymentManager:
  fullnameOverride: deployment-manager
  image: 
    repository: karma001/dmanager
    tag: dm_v1
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5000
  conf:
    logLevel: INFO
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: 2Gi
packageRepository:
  fullnameOverride: package-repository
  image: 
    repository: pnda/package-repository
    tag: 1.1.1
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8888
  conf:
    logLevel: INFO
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"
  persistence:
    enabled: true
    storageClass:
    accessMode: ReadWriteOnce
    size: 5Gi

jmxproxy:
  fullnameOverride: jmxproxy

testing:
  enabled: true
  fullnameOverride: testing
  schedule: "* * * * *"
  image: 
    repository: pnda/testing
    tag: "1.1.1"
    pullPolicy: IfNotPresent
  conf:
    logLevel: WARN
  resources:
    requests:
      memory: "128Mi"
      cpu: "10m"
    limits:
      memory: "1024Mi"
      cpu: "1000m"


############################
# Kafka related Components
############################
strimzi:
  enabled: true
  kafka:
    enabled: true
    replicas: 1
  zookeeper:
    replicas: 1
  kafkaConnect:
    enabled: true
    replicas: 1
    image: pnda/strimzi-connector:2.3.1

schemaregistry:
  enabled: true
  # confluent platform v5.3.2 to work with kafka 2.3.1
  imageTag: 5.3.2
  # we use schemaregistry instead of schema-registry to avoid kubernetes inject SCHEMA_REGISTRY_PORT env variable
  fullnameOverride: schemaregistry
  kafka:
    enabled: false
  kafkaStore:
    overrideBootstrapServers: "PLAINTEXT://strimzi-kafka-bootstrap:9092"

schemaregistry-ui:
  enabled: false
  fullnameOverride: schemaregistry-ui
  service:
    type: ClusterIP
  schemaRegistry:
    url: "http://schemaregistry"
    proxy: true
  ingress:
    enabled: true
    hosts:
      - schema-registry.127-0-0-1.nip.io

kafkaHdfsConnector: 
  enabled: true
  fullnameOverride: kafka-hdfs-connector

kafka-manager:
  enabled: true
  fullnameOverride: kafka-manager
  image:
    repository: gradiant/kafka-manager
    tag: 2.0.0.2
  zkHosts: "zoo-entrance:2181"
  clusters:
   - name: "pnda-Kafka"
     kafkaVersion: "2.2.0"
     jmxEnabled: "true"
     tuning: {}
  ingress:
    enabled: true
    hosts:
      - kafka-manager.127-0-0-1.nip.io

kafka-connect-ui:
  enabled: true
  fullnameOverride: kafka-connect-ui
  connectCluster:
    url: http://strimzi-connect-api:8083
  ingress:
    enabled: true
    hosts:
      - connect.127-0-0-1.nip.io


# Check https://zero-to-jupyterhub.readthedocs.io/en/latest/ for customize this section
jupyterhub:
  enabled: true
  hub:
    extraConfig:  
      # to resolve singleuser pods (jupyter-{username}.jupyterhub) through kubernetes DNS
      00-custom-singleuser-hostname: |
         c.KubeSpawner.extra_pod_config = {
            "hostname": "jupyter-{username}",
            "subdomain": "jupyterhub"
         }
      01-posthook: |
         c.KubeSpawner.lifecycle_hooks = {
            "postStart": {
              "exec": {
                "command": [
                  "sh",
                  "-c",
                  "rm -rf /home/jovyan/apps-notebooks && git clone http://deployment-manager-git:8099/jupyter-${JUPYTERHUB_USER} /home/jovyan/apps-notebooks || exit 0"]
              }
            }
          }
      jupyterlab: |
        c.Spawner.cmd = ['jupyter-labhub']
  auth:
    type: dummy
    dummy:
      password: pnda
    admin:
      users:
        - pnda
  proxy:
    # edit with your own 32 bytes secretToken (e.g. generate with openssl rand -hex 32)
    secretToken: "83fc7b97f79e48a88dd565397a165ebfa9053e474350bb338448b94c6b19c076"
    service:
      type: ClusterIP
      nodePorts: {}
  singleuser:
    # Workaround  for microk8s https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/1189#issuecomment-472601915
    cloudMetadata:
      enabled: true
    serviceAccountName: pnda
    image:
      # Check https://github.com/Gradiant/dockerized-jupyter/blob/master/Dockerfile
      name: gradiant/jupyter
      tag: 6.0.1-spark
    defaultUrl: "/lab"
    extraEnv:
      SPARKCONF_SPARK_MASTER: "spark://spark-standalone:7077"
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 64m
    hosts:
      - notebooks.127-0-0-1.nip.io

############################
# Hadoop related Components
############################
hdfs:
  enabled: true
  fullnameOverride: hdfs
  persistence:
    nameNode:
      enabled: true
      size: 4Gi
    dataNode:
      enabled: true
      size: 20Gi
  ingress:
    nameNode:
      enabled: true
      hosts:
      - "hdfs.127-0-0-1.nip.io"
    httpfs:
      enabled: true
      hosts:
      - "httpfs.127-0-0-1.nip.io"

hbase:
  enabled: true
  hdfs:
    enabled: false
  zookeeper:
    enabled: false
  fullnameOverride: hbase
  conf:
    hbaseSite:
      hbase.rootdir:  "hdfs://hdfs-namenode:8020/hbase"  # default is "{{.Release.Name}}-hdfs-namenode:8020"
      hbase.zookeeper.quorum: "zoo-entrance:2181"  # default is "{{.Release.Name}}-zookeeper:2181"

opentsdb:
  enabled: true
  hbase:
    enabled: false
  fullnameOverride: opentsdb
  hbaseConfigMapName: hbase # Default hbaseConfigMapName is {{ .Release.Name}}-hbase
  conf:
    tsd.storage.hbase.zk_quorum: "zoo-entrance:2181"  # default is "{{.Release.Name}}-cp-zookeeper:2181"
  

sparkoperator:
  enabled: true
  fullnameOverride: sparkoperator

# Setting spark-standalone worker instances to 1
spark-standalone:
  enabled: true
  fullnameOverride: spark-standalone
  ingress:
    enabled: true
    hosts:
      - spark.127-0-0-1.nip.io


############################
# Other Components
############################

grafana:
  enabled: true
  fullnameOverride: grafana
  image: 
    tag: 6.4.4
  sidecar:
    datasources:
      enabled: true
      # label that the configmaps with datasources are marked with
      label: grafana_datasource
  adminUser: "pnda"
  adminPassword: "pnda"
  ingress:
    enabled: true
    hosts:
      - grafana.127-0-0-1.nip.io

# Setting elasticsearch instances to 1
elasticsearch:
  enabled: false
  # default hard. set to soft to permit co-located instances.
  antiAffinity: "soft"
  # Shrink default JVM heap.
  esJavaOpts: "-Xmx256m -Xms256m"
  replicas: 1

kibana:
  enabled: false
  ingress:
    enabled: true
    hosts:
      - kibana.127-0-0-1.nip.io

